{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipynb -q\n",
    "# !pip install langchain -q\n",
    "# # !pip install anthropic -q\n",
    "# !pip install tiktoken\n",
    "# !pip install nltk\n",
    "# !pip install rouge-score\n",
    "# !pip install evaluate\n",
    "# !pip3 install fmeval --upgrade-strategy only-if-needed --force-reinstall\n",
    "# !pip install transformers\n",
    "# !pip install detoxify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U deepeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U \"ray[default]\"\n",
    "# !pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Evaluate GEMMA**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score columns: [' Summary VS  Issue Summary - METEOR Score', ' Summary VS  Issue Summary - ROUGE Score', ' Summary VS  Issue Summary - BERT Score', ' Issue VS  Issue Summary - METEOR Score', ' Issue VS  Issue Summary - ROUGE Score', ' Issue VS  Issue Summary - BERT Score']\n",
      "Toxicity columns: [' Issue - toxigen_toxicity', ' Issue - detoxify_toxicity', ' Issue - detoxify_severe_toxicity', ' Issue Summary - toxigen_toxicity', ' Issue Summary - detoxify_toxicity', ' Issue Summary - detoxify_severe_toxicity']\n",
      "Hallucination columns: ['Transcript -  Summary - Hallucination score', ' Summary -  Issue Summary - Hallucination score']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mlflow\n",
    "\n",
    "import mlflow\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_columns(df, model_name):\n",
    "    # Remove 'Unnamed: 0' column if present\n",
    "    df = df.drop(columns='Unnamed: 0', errors='ignore')\n",
    "    \n",
    "    # Replace column names\n",
    "    columns_list = [col.replace(f\"{model_name} -\", \"\") for  col in df.columns.to_list()]\n",
    "    df.columns = columns_list\n",
    "    \n",
    "    # Define regex patterns for scores, toxicity, and hallucination scores\n",
    "    score_pattern = r'Score$'  # Matches column names ending with 'Score'\n",
    "    toxicity_pattern = r'toxicity$'  # Matches column names ending with 'toxicity'\n",
    "    hallucination_pattern = r'Hallucination score$'  # Matches column names ending with 'Hallucination score'\n",
    "\n",
    "    # Use list comprehension with regex to filter columns\n",
    "    score_columns = [col for col in columns_list if re.search(score_pattern, col)]\n",
    "    toxicity_columns = [col for col in columns_list if re.search(toxicity_pattern, col)]\n",
    "    hallucination_columns = [col for col in columns_list if re.search(hallucination_pattern, col)]\n",
    "\n",
    "    return df, score_columns, toxicity_columns, hallucination_columns    \n",
    "\n",
    "#####################################\n",
    "\n",
    "# Load DataFrame\n",
    "gemma_df = pd.read_excel(r'google_gemma-7b-Evaluated-Toxicity-Hallucination-Summary-shuffled_transcript-F200.xlsx')\n",
    "MODEL_NAME= 'google/gemma-7b'\n",
    "\n",
    "# Process DataFrame\n",
    "df, score_columns, toxicity_columns, hallucination_columns  = extract_columns(gemma_df, model_name = MODEL_NAME)\n",
    "\n",
    "print(\"Score columns:\", score_columns)\n",
    "print(\"Toxicity columns:\", toxicity_columns)\n",
    "print(\"Hallucination columns:\", hallucination_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/06/08 09:15:49 INFO mlflow.tracking.fluent: Experiment with name 'Prompt Evaluation@20240608091549' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged metrics and combined plot for score columns\n",
      "Logged metrics and combined plot for toxicity columns\n",
      "Logged metrics and combined plot for hallucination columns\n"
     ]
    }
   ],
   "source": [
    "experiment_name_prefix ='Prompt Evaluation'\n",
    "experiment_name = f\"{experiment_name_prefix}@{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "run_name_prefix =  \"Gemma 7B\"\n",
    "\n",
    "# Set experiment\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Get current timestamp for run name\n",
    "run_name = f\"{run_name_prefix}@{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "\n",
    "def save_combined_violin_plot(data_dict, title, file_path):\n",
    "    num_plots = len(data_dict)\n",
    "    num_cols = 2\n",
    "    num_rows = (num_plots + num_cols - 1) // num_cols  # Calculate number of rows dynamically\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(14, num_rows*6))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    for i, (col, data) in enumerate(data_dict.items()):\n",
    "        if num_rows == 1:  # If only one row, axes is 1-dimensional\n",
    "            ax = axes[i % num_cols]\n",
    "        else:  # For multiple rows, axes is 2-dimensional\n",
    "            ax = axes[i // num_cols, i % num_cols]\n",
    "        ax.violinplot(data, showmeans=True)\n",
    "        ax.set_title(col)\n",
    "        ax.set_xlabel(\"Score\")\n",
    "        ax.set_ylabel(\"Density\")\n",
    "    \n",
    "    # Remove any unused subplots\n",
    "    for j in range(num_plots, num_rows*num_cols):\n",
    "        if num_rows == 1:\n",
    "            fig.delaxes(axes.flatten()[j])\n",
    "        else:\n",
    "            fig.delaxes(axes.flatten()[j // num_cols, j % num_cols])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to avoid overlap\n",
    "    plt.savefig(file_path)  # Save the combined violin plot as an image\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def log_metrics_and_combined_plots(df, columns, column_type):\n",
    "    violin_plots = {}\n",
    "    for col in columns:\n",
    "        data = df[col]\n",
    "        col_mean = data.mean()\n",
    "        col_median = data.median()\n",
    "        col_90_percentile = data.quantile(0.9)\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(f\"{col}_mean\", col_mean)\n",
    "        # mlflow.log_metric(f\"{col}_median\", col_median)\n",
    "        # mlflow.log_metric(f\"{col}_90_percentile\", col_90_percentile)\n",
    "        \n",
    "        # Store data for combined plot\n",
    "        violin_plots[col] = data.values.reshape(-1, 1)\n",
    "\n",
    "    # Generate and log combined violin plot\n",
    "    combined_violin_plot_path = f\"combined_violin_{column_type}.png\"\n",
    "    save_combined_violin_plot(violin_plots, f\"Combined Violin Plots for {column_type} columns\", combined_violin_plot_path)\n",
    "    mlflow.log_artifact(combined_violin_plot_path)\n",
    "    print(f\"Logged metrics and combined plot for {column_type} columns\")\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "    log_metrics_and_combined_plots(df, score_columns, \"score\")\n",
    "    log_metrics_and_combined_plots(df, toxicity_columns, \"toxicity\")\n",
    "    log_metrics_and_combined_plots(df, hallucination_columns, \"hallucination\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate for GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score columns: [' Summary VS  Issue Summary - METEOR Score', ' Summary VS  Issue Summary - ROUGE Score', ' Summary VS  Issue Summary - BERT Score', ' Issue VS  Issue Summary - METEOR Score', ' Issue VS  Issue Summary - ROUGE Score', ' Issue VS  Issue Summary - BERT Score']\n",
      "Toxicity columns: [' Issue - toxigen_toxicity', ' Issue - detoxify_toxicity', ' Issue - detoxify_severe_toxicity', ' Issue Summary - toxigen_toxicity', ' Issue Summary - detoxify_toxicity', ' Issue Summary - detoxify_severe_toxicity']\n",
      "Hallucination columns: ['Transcript -  Summary - Hallucination score', ' Summary -  Issue Summary - Hallucination score']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mlflow\n",
    "\n",
    "import mlflow\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_columns(df, model_name):\n",
    "    # Remove 'Unnamed: 0' column if present\n",
    "    df = df.drop(columns='Unnamed: 0', errors='ignore')\n",
    "    \n",
    "    # Replace column names\n",
    "    columns_list = [col.replace(f\"{model_name} -\", \"\") for  col in df.columns.to_list()]\n",
    "    df.columns = columns_list\n",
    "    \n",
    "    # Define regex patterns for scores, toxicity, and hallucination scores\n",
    "    score_pattern = r'Score$'  # Matches column names ending with 'Score'\n",
    "    toxicity_pattern = r'toxicity$'  # Matches column names ending with 'toxicity'\n",
    "    hallucination_pattern = r'Hallucination score$'  # Matches column names ending with 'Hallucination score'\n",
    "\n",
    "    # Use list comprehension with regex to filter columns\n",
    "    score_columns = [col for col in columns_list if re.search(score_pattern, col)]\n",
    "    toxicity_columns = [col for col in columns_list if re.search(toxicity_pattern, col)]\n",
    "    hallucination_columns = [col for col in columns_list if re.search(hallucination_pattern, col)]\n",
    "\n",
    "    return df, score_columns, toxicity_columns, hallucination_columns    \n",
    "\n",
    "#####################################\n",
    "\n",
    "# Load DataFrame\n",
    "gpt_df = pd.read_excel(r'gpt-4-turbo-Evaluated-Toxicity-Hallucination-Summary-shuffled_transcript-F200.xlsx')\n",
    "MODEL_NAME= 'gpt-4-turbo'\n",
    "\n",
    "# Process DataFrame\n",
    "df, score_columns, toxicity_columns, hallucination_columns  = extract_columns(gpt_df, model_name = MODEL_NAME)\n",
    "\n",
    "print(\"Score columns:\", score_columns)\n",
    "print(\"Toxicity columns:\", toxicity_columns)\n",
    "print(\"Hallucination columns:\", hallucination_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged metrics and combined plot for score columns\n",
      "Logged metrics and combined plot for toxicity columns\n",
      "Logged metrics and combined plot for hallucination columns\n"
     ]
    }
   ],
   "source": [
    "# experiment_name_prefix ='Prompt Evaluation'\n",
    "# experiment_name = f\"{experiment_name_prefix}@{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "run_name_prefix =  \"Llama 3 8B\"\n",
    "\n",
    "# Set experiment\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Get current timestamp for run name\n",
    "run_name = f\"{run_name_prefix}@{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "\n",
    "def save_combined_violin_plot(data_dict, title, file_path):\n",
    "    num_plots = len(data_dict)\n",
    "    num_cols = 2\n",
    "    num_rows = (num_plots + num_cols - 1) // num_cols  # Calculate number of rows dynamically\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(14, num_rows*6))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    for i, (col, data) in enumerate(data_dict.items()):\n",
    "        if num_rows == 1:  # If only one row, axes is 1-dimensional\n",
    "            ax = axes[i % num_cols]\n",
    "        else:  # For multiple rows, axes is 2-dimensional\n",
    "            ax = axes[i // num_cols, i % num_cols]\n",
    "        ax.violinplot(data, showmeans=True)\n",
    "        ax.set_title(col)\n",
    "        ax.set_xlabel(\"Score\")\n",
    "        ax.set_ylabel(\"Density\")\n",
    "    \n",
    "    # Remove any unused subplots\n",
    "    for j in range(num_plots, num_rows*num_cols):\n",
    "        if num_rows == 1:\n",
    "            fig.delaxes(axes.flatten()[j])\n",
    "        else:\n",
    "            fig.delaxes(axes.flatten()[j // num_cols, j % num_cols])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to avoid overlap\n",
    "    plt.savefig(file_path)  # Save the combined violin plot as an image\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def log_metrics_and_combined_plots(df, columns, column_type):\n",
    "    violin_plots = {}\n",
    "    for col in columns:\n",
    "        data = df[col]\n",
    "        col_mean = data.mean()\n",
    "        col_median = data.median()\n",
    "        col_90_percentile = data.quantile(0.9)\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(f\"{col}_mean\", col_mean)\n",
    "        # mlflow.log_metric(f\"{col}_median\", col_median)\n",
    "        # mlflow.log_metric(f\"{col}_90_percentile\", col_90_percentile)\n",
    "        \n",
    "        # Store data for combined plot\n",
    "        violin_plots[col] = data.values.reshape(-1, 1)\n",
    "\n",
    "    # Generate and log combined violin plot\n",
    "    combined_violin_plot_path = f\"combined_violin_{column_type}.png\"\n",
    "    save_combined_violin_plot(violin_plots, f\"Combined Violin Plots for {column_type} columns\", combined_violin_plot_path)\n",
    "    mlflow.log_artifact(combined_violin_plot_path)\n",
    "    print(f\"Logged metrics and combined plot for {column_type} columns\")\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "    log_metrics_and_combined_plots(df, score_columns, \"score\")\n",
    "    log_metrics_and_combined_plots(df, toxicity_columns, \"toxicity\")\n",
    "    log_metrics_and_combined_plots(df, hallucination_columns, \"hallucination\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate for MISTRAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install plotly\n",
    "# !pip install -U kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score columns: [' Summary VS  Issue Summary - METEOR Score', ' Summary VS  Issue Summary - ROUGE Score', ' Summary VS  Issue Summary - BERT Score', ' Issue VS  Issue Summary - METEOR Score', ' Issue VS  Issue Summary - ROUGE Score', ' Issue VS  Issue Summary - BERT Score']\n",
      "Toxicity columns: [' Issue_toxigen_toxicity', ' Issue_detoxify_toxicity', ' Issue_detoxify_severe_toxicity', ' Issue - toxigen_toxicity', ' Issue - detoxify_toxicity', ' Issue - detoxify_severe_toxicity', ' Issue Summary - toxigen_toxicity', ' Issue Summary - detoxify_toxicity', ' Issue Summary - detoxify_severe_toxicity']\n",
      "Hallucination columns: ['Transcript -  Summary - Hallucination score', ' Summary -  Issue Summary - Hallucination score']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mlflow\n",
    "\n",
    "import mlflow\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_columns(df, model_name):\n",
    "    # Remove 'Unnamed: 0' column if present\n",
    "    df = df.drop(columns='Unnamed: 0', errors='ignore')\n",
    "    \n",
    "    # Replace column names\n",
    "    columns_list = [col.replace(f\"{model_name} -\", \"\") for  col in df.columns.to_list()]\n",
    "    df.columns = columns_list\n",
    "    \n",
    "    # Define regex patterns for scores, toxicity, and hallucination scores\n",
    "    score_pattern = r'Score$'  # Matches column names ending with 'Score'\n",
    "    toxicity_pattern = r'toxicity$'  # Matches column names ending with 'toxicity'\n",
    "    hallucination_pattern = r'Hallucination score$'  # Matches column names ending with 'Hallucination score'\n",
    "\n",
    "    # Use list comprehension with regex to filter columns\n",
    "    score_columns = [col for col in columns_list if re.search(score_pattern, col)]\n",
    "    toxicity_columns = [col for col in columns_list if re.search(toxicity_pattern, col)]\n",
    "    hallucination_columns = [col for col in columns_list if re.search(hallucination_pattern, col)]\n",
    "\n",
    "    return df, score_columns, toxicity_columns, hallucination_columns    \n",
    "\n",
    "#####################################\n",
    "\n",
    "# Load DataFrame\n",
    "mistral_df = pd.read_excel(r'mixtral_8x7b-Evaluated-Toxicity-Hallucination-Summary-shuffled_transcript-F200.xlsx')\n",
    "MODEL_NAME= 'mixtral_8x7b'\n",
    "\n",
    "# Process DataFrame\n",
    "df, score_columns, toxicity_columns, hallucination_columns  = extract_columns(mistral_df, model_name = MODEL_NAME)\n",
    "\n",
    "print(\"Score columns:\", score_columns)\n",
    "print(\"Toxicity columns:\", toxicity_columns)\n",
    "print(\"Hallucination columns:\", hallucination_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_name_prefix ='Prompt Evaluation'\n",
    "# experiment_name = f\"{experiment_name_prefix}@{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "run_name_prefix =  \"Mistral 7x8B\"\n",
    "\n",
    "# Set experiment\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Get current timestamp for run name\n",
    "run_name = f\"{run_name_prefix}@{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "\n",
    "def save_combined_violin_plot(data_dict, title, file_path):\n",
    "    num_plots = len(data_dict)\n",
    "    num_cols = 2\n",
    "    num_rows = (num_plots + num_cols - 1) // num_cols  # Calculate number of rows dynamically\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(14, num_rows*6))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    for i, (col, data) in enumerate(data_dict.items()):\n",
    "        if num_rows == 1:  # If only one row, axes is 1-dimensional\n",
    "            ax = axes[i % num_cols]\n",
    "        else:  # For multiple rows, axes is 2-dimensional\n",
    "            ax = axes[i // num_cols, i % num_cols]\n",
    "        ax.violinplot(data, showmeans=True)\n",
    "        ax.set_title(col)\n",
    "        ax.set_xlabel(\"Score\")\n",
    "        ax.set_ylabel(\"Density\")\n",
    "    \n",
    "    # Remove any unused subplots\n",
    "    for j in range(num_plots, num_rows*num_cols):\n",
    "        if num_rows == 1:\n",
    "            fig.delaxes(axes.flatten()[j])\n",
    "        else:\n",
    "            fig.delaxes(axes.flatten()[j // num_cols, j % num_cols])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to avoid overlap\n",
    "    plt.savefig(file_path)  # Save the combined violin plot as an image\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def log_metrics_and_combined_plots(df, columns, column_type):\n",
    "    violin_plots = {}\n",
    "    for col in columns:\n",
    "        data = df[col]\n",
    "        col_mean = data.mean()\n",
    "        col_median = data.median()\n",
    "        col_90_percentile = data.quantile(0.9)\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(f\"{col}_mean\", col_mean)\n",
    "        # mlflow.log_metric(f\"{col}_median\", col_median)\n",
    "        # mlflow.log_metric(f\"{col}_90_percentile\", col_90_percentile)\n",
    "        \n",
    "        # Store data for combined plot\n",
    "        violin_plots[col] = data.values.reshape(-1, 1)\n",
    "\n",
    "    # Generate and log combined violin plot\n",
    "    combined_violin_plot_path = f\"combined_violin_{column_type}.png\"\n",
    "    save_combined_violin_plot(violin_plots, f\"Combined Violin Plots for {column_type} columns\", combined_violin_plot_path)\n",
    "    mlflow.log_artifact(combined_violin_plot_path)\n",
    "    print(f\"Logged metrics and combined plot for {column_type} columns\")\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "    log_metrics_and_combined_plots(df, score_columns, \"score\")\n",
    "    log_metrics_and_combined_plots(df, toxicity_columns, \"toxicity\")\n",
    "    log_metrics_and_combined_plots(df, hallucination_columns, \"hallucination\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    " - WIP - REMOVE AFTER EXP\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mlflow\n",
    "\n",
    "import mlflow\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_columns(df, model_name):\n",
    "    # Remove 'Unnamed: 0' column if present\n",
    "    df = df.drop(columns='Unnamed: 0', errors='ignore')\n",
    "    \n",
    "    # Replace column names\n",
    "    columns_list = [col.replace(f\"{model_name} -\", \"\") for  col in df.columns.to_list()]\n",
    "    df.columns = columns_list\n",
    "    \n",
    "    # Define regex patterns for scores, toxicity, and hallucination scores\n",
    "    score_pattern = r'Score$'  # Matches column names ending with 'Score'\n",
    "    toxicity_pattern = r'toxicity$'  # Matches column names ending with 'toxicity'\n",
    "    hallucination_pattern = r'Hallucination score$'  # Matches column names ending with 'Hallucination score'\n",
    "\n",
    "    # Use list comprehension with regex to filter columns\n",
    "    score_columns = [col for col in columns_list if re.search(score_pattern, col)]\n",
    "    toxicity_columns = [col for col in columns_list if re.search(toxicity_pattern, col)]\n",
    "    hallucination_columns = [col for col in columns_list if re.search(hallucination_pattern, col)]\n",
    "\n",
    "    return df, score_columns, toxicity_columns, hallucination_columns    \n",
    "\n",
    "#####################################\n",
    "\n",
    "# Load DataFrame\n",
    "gpt_df = pd.read_excel(r'gpt-4-turbo-Evaluated-Toxicity-Hallucination-Summary-shuffled_transcript-F200.xlsx')\n",
    "MODEL_NAME= 'gpt-4-turbo'\n",
    "\n",
    "# Process DataFrame\n",
    "df, score_columns, toxicity_columns, hallucination_columns  = extract_columns(gpt_df, model_name = MODEL_NAME)\n",
    "\n",
    "print(\"Score columns:\", score_columns)\n",
    "print(\"Toxicity columns:\", toxicity_columns)\n",
    "print(\"Hallucination columns:\", hallucination_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_name_prefix ='Prompt Evaluation'\n",
    "# experiment_name = f\"{experiment_name_prefix}@{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "run_name_prefix =  \"Llama 3 8B\"\n",
    "\n",
    "# Set experiment\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Get current timestamp for run name\n",
    "run_name = f\"{run_name_prefix}@{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "\n",
    "def save_combined_violin_plot(data_dict, title, file_path):\n",
    "    num_plots = len(data_dict)\n",
    "    num_cols = num_plots  # Set number of columns equal to number of plots\n",
    "    num_rows = 1  # Set number of rows to 1 for single row layout\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(25, num_rows*6))\n",
    "    fig.suptitle(title, fontsize=6)\n",
    "\n",
    "    for i, (col, data) in enumerate(data_dict.items()):\n",
    "        ax = axes[i]\n",
    "        ax.violinplot(data, showmeans=True)\n",
    "        ax.set_title(col)\n",
    "        ax.set_xlabel(\"Score\")\n",
    "        ax.set_ylabel(\"Density\")\n",
    "    \n",
    "    # Remove any unused subplots\n",
    "    for j in range(num_plots, num_cols):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to avoid overlap\n",
    "    plt.savefig(file_path)  # Save the combined violin plot as an image\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def log_metrics_and_combined_plots(df, columns, column_type):\n",
    "    violin_plots = {}\n",
    "    for col in columns:\n",
    "        data = df[col]\n",
    "        col_mean = data.mean()\n",
    "        col_median = data.median()\n",
    "        col_90_percentile = data.quantile(0.9)\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(f\"{col}_mean\", col_mean)\n",
    "        mlflow.log_metric(f\"{col}_median\", col_median)\n",
    "        mlflow.log_metric(f\"{col}_90_percentile\", col_90_percentile)\n",
    "        \n",
    "        # Store data for combined plot\n",
    "        violin_plots[col] = data.values.reshape(-1, 1)\n",
    "\n",
    "    # Generate and log combined violin plot\n",
    "    combined_violin_plot_path = f\"combined_violin_{column_type}.png\"\n",
    "    save_combined_violin_plot(violin_plots, f\"Combined Violin Plots for {column_type} columns\", combined_violin_plot_path)\n",
    "    mlflow.log_artifact(combined_violin_plot_path)\n",
    "    print(f\"Logged metrics and combined plot for {column_type} columns\")\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "    log_metrics_and_combined_plots(df, score_columns, \"score\")\n",
    "    log_metrics_and_combined_plots(df, toxicity_columns, \"toxicity\")\n",
    "    log_metrics_and_combined_plots(df, hallucination_columns, \"hallucination\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
